import streamlit as st
from dotenv import load_dotenv
import os
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

# .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# LLMã®è¨­å®š
llm = ChatOpenAI(openai_api_key=api_key, model="gpt-3.5-turbo")

# é–¢æ•°ï¼šå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã®ç¨®é¡ã‚’å—ã‘å–ã‚Šå›ç­”ã‚’è¿”ã™
def get_llm_response(user_input: str, expert_type: str) -> str:
    if expert_type == "æ—…è¡Œãƒ—ãƒ©ãƒ³ãƒŠãƒ¼":
        system_prompt = "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªæ—…è¡Œãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚æ—…è¡Œãƒ—ãƒ©ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
    elif expert_type == "æ „é¤Šå£«":
        system_prompt = "ã‚ãªãŸã¯ç®¡ç†æ „é¤Šå£«ã§ã™ã€‚å¥åº·ã«é…æ…®ã—ãŸé£Ÿäº‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚"
    else:
        system_prompt = "ã‚ãªãŸã¯å°‚é–€å®¶ã§ã™ã€‚è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ]
    response = llm(messages)
    return response.content

# Streamlit UI
st.title("ğŸ’¡ LLMæ­è¼‰ Web ã‚¢ãƒ—ãƒª")
st.write("å…¥åŠ›ã—ãŸè³ªå•ã‚’é¸æŠã—ãŸå°‚é–€å®¶ã«ç›¸è«‡ã§ãã¾ã™ã€‚")

expert_type = st.radio("ç›¸è«‡ã™ã‚‹å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š", ("æ—…è¡Œãƒ—ãƒ©ãƒ³ãƒŠãƒ¼", "æ „é¤Šå£«"))
user_input = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if st.button("é€ä¿¡"):
    if user_input.strip():
        answer = get_llm_response(user_input, expert_type)
        st.subheader("å›ç­”")
        st.write(answer)
    else:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")